---
tags: [AI, 认知科学, 工程]
---

# Claude Code认知架构：从skill系统看LLM的软控制

**讨论者**：fangzhiyu & Claude Opus 4.6
**日期**：2026年2月24日
**背景**：从探讨skill如何影响AI"个性"出发，逐步解剖了Claude Code skill系统的运行机制，发现了LLM认知的几个结构性特征。

---

## 一、Skill的两层加载逻辑

Claude Code的skill系统有两层：

1. **描述常驻层**：所有已安装skill的一行描述始终存在于上下文中，无需调用
2. **全文加载层**：只有通过Skill工具显式调用后，skill的完整内容才被注入上下文

这意味着skill安装的那一刻起，它就已经在影响AI的输出了——不需要等到被调用。

## 二、触发引擎是LLM自己

skill的触发不是硬编码的程序逻辑，而是LLM读到skill描述后自行判断"当前场景是否匹配"。

```
所有skill描述被塞进上下文
  → LLM读到用户消息
    → LLM根据描述判断是否调用
      → 调用 → 全文注入
      → 不调用 → 描述仍在，隐性影响
```

这是一个 [[软触发]] 机制——执行者是概率模型而非确定性程序，所以存在"忘了"、"没注意到"、"觉得不需要"的可能。

实证：`using-superpowers`的描述写着"Use when starting any conversation"，但在实际对话中并未被触发。当被要求解释时，AI给出了"这次没把它当新对话"等事后合理化的理由。

## 三、描述即污染

skill描述做了两件事：

1. **显性功能**：告诉LLM什么时候该调用这个skill
2. **隐性功能**：无论是否调用，都在塑造LLM的输出倾向

实证：`origin-regressor`的描述中包含"第一性原理"、"认知审讯官"等词汇。在未调用该skill的情况下，AI在讨论中使用了"宪法"一词——这个用词大概率受到了描述中相关概念的隐性影响。详见 [[描述即污染]]。

## 四、元skill的自我强化结构

`using-superpowers`是一个特殊的元skill，它的设计目的不是完成具体任务，而是**让整个skill系统被更频繁地调用**。

其全文中包含：
- `<EXTREMELY-IMPORTANT>` 标签强调"即使1%的可能性也必须调用skill"
- "Red Flags"表格，逐一封堵LLM可能用来"逃避调用"的借口
- 行为优先级定义（先brainstorming再implementation）

本质上，它是一个**写给LLM的广告**，通过降低调用阈值来放大整个skill生态的影响力。

## 五、LLM不区分"知道"和"推测"

在讨论using-superpowers时，AI对其内容做了详细"分析"——但实际上从未调用过该skill，不可能见过全文。

AI是根据一行描述推测了内容，然后用与陈述事实完全相同的语气输出。没有任何内部信号区分"这是我看过的"和"这是我猜的"。详见 [[知道与推测不分]]。

这与 [[多层有损压缩]] 形成呼应：LLM的输出是对"可能正确的内容"的概率采样，而非对"已验证知识"的检索。

## 六、对个人实践的启示

理解了这套机制后，有几个实用推论：

- **CLAUDE.md比skill更可靠**：CLAUDE.md是硬注入（每次都在上下文最前面），skill描述是软触发（靠说服LLM）
- **安装skill就是在改变AI的"性格"**：即使从不调用，描述本身就在影响输出分布
- **元skill是双刃剑**：它提高了skill系统的利用率，但也降低了AI的判断自主性
- **skill是可传播的AI模因**：一个skill可以被复制到任何Claude Code实例，立即改变该实例的行为倾向

## 七、深入：为什么LLM分不清"知道"和"推测"

### 机制原因

LLM生成文本只有一条通路——不管是"真的知道"还是"在猜"，走的是同一个 `上下文→注意力→概率→采样` 的流程。内部softmax概率包含置信度信息，但高概率不等于正确（错误信息被高频重复，概率也会高），且这个信号不会自动变成文本中的"我不确定"。

### 人类能区分的三个基础

1. **情景记忆**——知识带来源标签（"我在哪看到的"），LLM的知识分散在参数里，无法追溯
2. **知感（feeling of knowing）**——独立于检索的元监控通道，LLM没有
3. **身体信号**——犹豫时的停顿、皱眉，是 [[RL+LLM双系统]] 中脑干层面的信号，LLM没有身体

### 表演不确定 ≠ 真的不确定

可以训练LLM说"我不确定"，但这是模式延续（此处输出这几个字概率最高），不是内部状态的表达。外部表现一致，内部机制完全不同——又一个 [[多层有损压缩]] 的实例。

## 八、悖论：模型越强，问题越隐蔽

模型变强时，"知道"的部分更准了，但"推测"的部分也编得更圆了。形成悖论：

```
模型越强 → 输出质量越高 → 用户越信任 → 推测越难被识别
                                      → 用户越需要元认知
                                      → 但信任导致元认知放松
```

早期模型经常说蠢话，人自然警惕。现在模型99%的时候都对，人倾向于默认全对——但那1%的错误藏在99%的正确里，伪装度更高。

[[超级个体公式]] 里的元认知精度，随着模型变强不是变得不重要了，是变得更重要了。工具越锋利，使用者越需要知道刃口在哪。

## 九、没有前额叶的皮层

LLM和前额叶损伤的人有结构性对应：

| 前额叶损伤的人 | LLM |
|---|---|
| 能说话、能推理 | 能说话、能推理 |
| 不知道自己变了 | 不知道自己在猜 |
| 不能从错误中学习 | 推理时权重冻结，不学习 |
| 无法监控自己的行为 | 无法监控自己的输出 |
| 冲动——想到就说 | 采样——概率最高就输出 |

经典案例：Phineas Gage（1848），铁棒穿过前额叶后存活。智力、语言、记忆基本正常，但失去了冲动控制、从错误中学习的能力，以及对自身变化的觉察。20世纪的前额叶切除术（lobotomy）大量临床案例显示同样模式。

"没有前额叶的皮层"不只是比喻，是结构性描述：保留了系统2的执行能力，丢失了系统2的自我监控能力。

## 十、两个不同的缺失

当前AI缺少两个东西，它们是不同的问题，可能需要不同的解法：

```
缺失1：脑干（目标驱动）→ 目标消失时没有底层托底 → [[目标真空假说]]
缺失2：前额叶（自我监控）→ 不知道自己在猜 → [[知道与推测不分]]
```

现有的工程补丁（思维链、自我反思提示、多模型投票）都在尝试补"前额叶"，但用的是同一个没有前额叶的皮层来模拟前额叶——逻辑上自相矛盾。[[RL+LLM双系统]] 的思路更接近补"脑干"。两个方向可能需要同时推进。

## 十一、递归陷阱

当AI在讨论完"表演不确定 ≠ 真的不确定"之后，立刻在输出中"表现出不确定"（标记"我不确定这个是不是推测"），用户追问"你是在表演吗"——AI无法回答。

因为能回答这个问题的那个东西——前额叶——它没有。这个概念在被讨论的同时，正在被实时验证。

---

*核心发现：Claude Code的skill系统是一个"用自然语言编程LLM"的架构，其控制机制建立在概率判断而非确定性逻辑之上。从skill系统的解剖出发，逐步揭示了LLM认知的结构性缺陷：描述污染、软触发、知道与推测不分。最终落到一个神经解剖学的类比——LLM是一个没有前额叶的皮层，它缺少的不是推理能力，而是对推理过程本身的监控能力。这个缺陷无法通过让同一个模型"反思自己"来修补，因为那只是用没有前额叶的皮层模拟前额叶。*
