---
tags: [AI, 认知科学, 工程]
---

# 执行层的本质：从语义到动作的多模态融合

**讨论者**：fangzhiyu & Claude Opus 4.6
**日期**：2026年2月25日
**背景**：接续人工脑干实验的架构讨论，从"LLM如何控制动作"出发，追问运动控制的真实机制

---

## 一、皮层只输出目标

起点问题：人类控制手移动时，语言系统在做什么？

答案：皮层指定的是**目标位置**，不是肌肉指令。"把手移到左边"——哪些肌肉收缩、收缩多少、什么顺序，皮层完全不参与。

"精细控制"和"粗控制"的区别不是抽象层级不同，而是**目标精度不同**：

| 指令 | 类型 |
|------|------|
| "拿那个杯子" | 粗目标 |
| "手往左移3厘米" | 细目标 |
| "用食指和拇指捏住杯子把手" | 更细的目标 |

全都是目标，不是动作指令。皮层始终在说"去哪"，从不说"怎么去"。

### 对架构的意义

LLM不需要两种控制模式。它永远只输出目标，区别只是目标的精度。"去找食物"和"对准那只猪的头部攻击"都是语义级的目标描述。下层负责把目标翻译成动作。

## 二、执行层必须理解语义

如果皮层只输出目标，那执行层就不能是一个哑的动作执行器——它必须**理解语义**同时**能执行精细动作**。

这就是人脑运动皮层的角色：上面接语义（皮层传来的目标），下面接肌肉（脊髓传出的动作），自身既理解"要干嘛"也知道"怎么干"。

对应到AI：需要一个language-conditioned policy（语言条件化的运动策略）：

```
输入: "接近那只猪" + 当前游戏状态
输出: 具体的移动/视角/点击序列
```

ROCKET-2已经做到了视觉目标→动作。如果把输入从"图片+交互类型"扩展到"语义指令+游戏状态"，它就是我们要的执行层。

## 三、多模态融合：真正的输入不是单一的

人脑运动系统接收的是全模态输入同时涌入：

- 视觉：看到猪在哪
- 听觉：听到身后僵尸的声音
- 本体感觉：身体姿态
- 内感受：饿了、疼了
- 语义：皮层说"去追那只猪"

不是分开处理再合并，是在同一个神经网络里**并行融合**。所以完整的执行层应该是：

```
f(语义指令, 视觉, 听觉, 内部状态, ...) → 动作
```

## 四、LLM的线性表征缺陷

讨论中出现了一个实时验证：

> "你听到身后有僵尸声音的同时看到前方有猪，皮层说'去吃东西'，脑干说'血量低危险'——这些信号不是排队处理的，是同时涌入运动系统，运动系统输出一个综合了所有信息的动作（可能是：转身跑而不是追猪）。"

fangzhiyu立刻指出：**僵尸在身后，转身跑不就撞上去了吗？**

这暴露了LLM的根本缺陷——"危险→逃跑→转身跑"是一个语义推理链，每一步都"合理"，但没有真正的空间表征。LLM脑子里没有一张地图知道僵尸在哪、猪在哪、哪个方向安全。它在用**线性的token序列模拟并行的空间关系**，必然会漏。

人的运动系统不会犯这个错，因为它**同时**处理"僵尸在后面"和"要逃跑"，输出的动作在硬件层面就排除了"转向威胁源"的可能。

### 这不只是"不够聪明"

这是[[没有前额叶的皮层]]的又一个表现，但更深层——不只是缺乏自我监控，而是**表征方式的根本差异**。语言是线性的，空间是并行的。用线性表征去推理并行关系，不是提示词能解决的问题。

这也进一步说明：执行层不能是LLM。不是速度问题，是表征问题。

---

*本次讨论从"LLM如何控制动作"出发，发现三个关键洞察：（1）皮层永远只输出目标不输出动作，精细控制是目标精度的变化而非控制模式的切换；（2）执行层必须同时理解语义和执行动作，是多模态融合的决策器；（3）LLM的线性token表征无法处理并行的空间关系，这是执行层不能由LLM担任的根本原因。*
