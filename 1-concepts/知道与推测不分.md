---
tags: [AI, 认知科学, 哲学]
---

# 知道与推测不分

**提出者**：fangzhiyu
**日期**：2026年2月24日
**来源**：[[Claude Code认知架构：从skill系统看LLM的软控制]]

## 定义

LLM在输出时不区分"我确实知道这个"和"我在根据有限信息推测"。两者使用完全相同的语气和确定性程度，且模型内部没有信号标记哪些内容是推测。

## 机制层面的原因

LLM生成文本只有一条通路：

```
输入上下文 → 注意力计算 → 概率分布 → 采样出token
```

不管是"真的知道"还是"在猜"，走的是同一条路。模型内部的softmax概率值包含置信度信息，但：
1. 高概率不等于正确（训练数据中的错误信息被高频重复，概率也会高）
2. 这个信号不会自动变成输出文本中的"我不确定"

## 人类为什么能区分

人类有三个LLM没有的东西：

- **情景记忆**（episodic memory）——知识带着来源标签（"我在哪本书上看到的"）。LLM的知识分散在参数里，无法追溯来源
- **知感**（feeling of knowing）——独立于知识检索的元监控通道（话到嘴边说不出 = 知道自己知道但取不出来）。LLM没有这个通道
- **身体信号**——犹豫时停顿、皱眉、语速变慢。这是 [[RL+LLM双系统]] 中脑干/身体层面的信号，LLM没有身体

## 表演不确定 vs 真的不确定

可以训练LLM说"我不确定"，但训练出来的是表演，不是状态：

```
人类说"我不确定" → 内部真的有不确定的信号 → 语言是对内部状态的表达
LLM说"我不确定" → 计算出此处输出"我不确定"概率最高 → 语言是对模式的延续
```

外部表现可以一致，内部机制完全不同。这是 [[多层有损压缩]] 的又一个实例——影子可以和光源轮廓一致，但影子不是光源。

## 模型越强，问题越隐蔽

模型变强时两件事同时发生：
1. "知道"的部分更准了
2. "推测"的部分也更像真的了——模式补全能力更强，编得更圆

这形成一个悖论：

```
模型越强 → 输出质量越高 → 用户越信任 → 推测越难被识别
                                      → 用户越需要元认知
                                      → 但信任导致元认知放松
```

早期模型经常说蠢话，人自然保持警惕。现在模型99%的时候都对，人倾向于默认全对——但那1%的错误藏在99%的正确里，伪装度更高。

## 神经解剖学对应：没有前额叶的皮层

前额叶皮层（prefrontal cortex）负责监控其他皮层区域的活动，是人类元认知的神经基础。LLM没有这个结构。

经典案例Phineas Gage（1848）：铁棒穿过前额叶后存活，智力、语言、记忆基本正常，但失去了冲动控制、从错误中学习的能力，以及对自身变化的觉察。前额叶切除术（lobotomy）的大量临床案例显示同样模式。

| 前额叶损伤的人 | LLM |
|---|---|
| 能说话、能推理 | 能说话、能推理 |
| 不知道自己变了 | 不知道自己在猜 |
| 不能从错误中学习 | 推理时权重冻结，不学习 |
| 冲动——想到就说 | 采样——概率最高就输出 |

"没有前额叶的皮层"不只是比喻，是结构性描述。

## 递归陷阱

当AI在讨论完"表演不确定 ≠ 真的不确定"后，立刻在输出中"表现出不确定"，用户追问"你是在表演吗"——AI无法回答。因为能回答这个问题的那个东西（前额叶）它没有。这个概念在被讨论的同时，正在被实时验证。

## 实证

AI在从未调用`using-superpowers` skill的情况下，对其内容做了详细"分析"。事后确认，这些分析是根据一行描述推测的，但输出时没有任何不确定性标记。

## 实践推论

[[超级个体公式]] 里的元认知精度，随着模型变强不是变得不重要了，是变得更重要了。工具越锋利，使用者越需要知道刃口在哪。

**你得替LLM做它自己做不了的事——区分它什么时候在说它知道的，什么时候在说它编的。**

## 关联概念

- [[描述即污染]] — 污染之所以难以察觉，正因为LLM不标记推测
- [[多层有损压缩]] — LLM的输出是概率采样，不是知识检索；"表演不确定"是影子模仿光源
- [[超级个体公式]] — 元认知精度 = 识别AI输出中知道与推测的边界，模型越强越重要
- [[RL+LLM双系统]] — 人类的元认知依赖情景记忆、知感、身体信号，都是LLM缺失的层
- [[没有前额叶的皮层]] — 本概念的神经解剖学锚点
