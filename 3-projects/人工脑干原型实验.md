---
tags: [AI, 认知科学, 工程, 实验]
---

# 人工脑干原型实验

**状态**：构想阶段
**来源**：[[人工脑干：从目标真空到RL+LLM双系统架构]]

## 目标

在Minecraft环境中验证"脑干层"的可行性 — 一个监控生存指标、能覆盖皮层决策的模块。

## 核心类比：给老鼠装人脑

- **老鼠身体**（JarvisVLA）：有固定的本能动作（跑、躲、觅食），不学新动作，但现有能力足够生存
- **人脑**（LLM皮层）：利用老鼠已有的本能做更聪明的决策（什么时候跑、往哪跑、先吃还是先躲）
- **脑干**（待建）：监控生存指标，必要时覆盖皮层决策

## 技术架构

```
┌─────────────────────────────────────┐
│  LLM 皮层（Opus 4.6）               │
│  规划、推理、长期决策                │
│  输出：语义级目标（精度可变）          │
│                                     │
│  特性：慢（秒级），能推理             │
│  角色：决策者 + 否决者（Libet意义上） │
└──────────┬──────────────────────────┘
           │ 语义指令
┌──────────▼──────────────────────────┐
│  技能缓存（Voyager式技能库）          │
│  已编译的决策模式 → 直接匹配执行      │
│  未命中 → 交回皮层推理               │
│                                     │
│  性质：符号化，中间态                 │
│  终态：→ 神经级技能编译（待解决）      │
└──────────┬──────────────────────────┘
           │ 语义指令（可被脑干中断/覆盖）
┌──────────▼──────────────────────────┐
│  脑干层（待实现）                     │
│  输入：血量/饥饿/威胁距离/昼夜         │
│  输出：不中断 / 撤退 / 进食 / 找掩体   │
│                                     │
│  特性：快（毫秒级），不需要语言能力     │
│  角色：生存守门员，先于皮层反应         │
│  参见：[[行动先于意识]]               │
│                                     │
│  演进：LLM探索 → 蒸馏到小模型          │
└──────────┬──────────────────────────┘
           │ 最终指令
┌──────────▼──────────────────────────┐
│  JarvisVLA（脊髓/执行层）             │
│  输入：语义指令 + 视觉 + 游戏状态      │
│  输出：键鼠动作序列                   │
│                                     │
│  特性：预训练固定，不学新动作           │
│  角色：老鼠的身体                     │
│                                     │
│  终态：→ 可持续学习的小脑（待解决）     │
└─────────────────────────────────────┘
```

### 架构说明

- JarvisVLA当前定位是**脊髓**（固定运动执行），不是小脑。真正的小脑（技能编译终点站）在当前架构中缺失，对应[[连续基质问题]]
- 技能缓存是[[技能编译]]的符号化中间态，用Voyager式代码库绕开[[连续基质问题]]
- 脑干层必须快于皮层反应（[[行动先于意识]]），LLM脑干只是探索阶段的临时方案

## 第一阶段：LLM脑干 + LLM皮层 + JarvisVLA

用LLM临时扮演脑干，探索脑干需要做什么。

### 环境
- Minecraft（MineStudio模拟器）
- 生存模式，难度正常

### 三层分工
- **皮层LLM**：接收状态摘要+任务目标，输出语义级行动计划
- **脑干LLM**：每tick接收生存指标（血量、饥饿、周围威胁、时间），判断是否需要覆盖皮层决策
- **JarvisVLA**：接收最终语义指令，输出键鼠操作

### 观察指标
- 脑干在什么条件下发出中断
- 皮层被中断后的行为变化
- 是否涌现出节律性生存行为（天黑找掩体、定期进食）
- 脑干的决策模式能否归纳为有限规则

## 第二阶段：蒸馏到小模型

记录LLM脑干的所有（输入状态→决策）数据，用监督学习蒸馏到小型分类器/MLP。脑干本质是分类任务（数值输入→有限动作类别），不需要语言能力，小模型足矣。可跳过"规则提炼"直接蒸馏。

## 第三阶段：技能缓存 + 持续优化

引入Voyager式符号化技能库，缓存皮层的成功决策模式。观察哪些技能能被符号化、哪些不能，为未来的神经级[[技能编译]]划定边界。如果用RL优化脑干，LLM皮层可反向加速训练（写奖励函数、设计课程、提供示范轨迹 — 康复训练类比）。

## 远期扩展：皮层-脑干双向竞争

当前架构只有单向中断（脑干覆盖皮层）。更完整的模型应包含反向：皮层否决脑干（如脑干提议进食，皮层判断有危险而压制）。

近似方案：给脑干动作加执行延迟窗口，紧急动作直接执行，非紧急动作等待皮层否决。对应Libet的"自由否决"概念（[[行动先于意识]]）。

此机制会显著增加系统复杂度，留到基础架构验证后再引入。

## 待解决

- JarvisVLA运行环境搭建（Linux/Docker，GPU要求）
- LLM API成本评估（双LLM每tick两次调用）
- [[连续基质问题]] — 分离架构下[[技能编译]]的近似方案（蒸馏、软切换、共享表征空间），留到后续阶段
- [[皮层只输出目标]] — LLM输出的目标精度如何与JarvisVLA的能力匹配
- JarvisVLA指令边界测试 — 不同粒度的语义指令（粗/中/细目标）执行能力验证，决定技能缓存的设计粒度

## 前置依赖

建议先完成 [[脑干实验ABC对照]]，验证目标真空假说后再推进。但可并行准备环境。

## 关联概念

- [[目标真空假说]]
- [[RL+LLM双系统]]
- [[自由能原理]]
- [[技能编译]]
- [[连续基质问题]]
- [[皮层只输出目标]]
- [[没有前额叶的皮层]]
- [[行动先于意识]]

## 关联文档

- [[人工脑干：从目标真空到RL+LLM双系统架构]]
- [[从Minecraft到技能编译：人工脑干的实现路径探索]]
- [[执行层的本质：从语义到动作的多模态融合]]
